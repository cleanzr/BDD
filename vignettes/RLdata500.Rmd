---
title: "Introduction to BDD"
author: "Neil Marchant"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to BDD}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

We demonstrate how to perform entity resolution using the `RLdata500` test 
data set included with the package. `RLdata500` contains 500 synthetic 
personal records, 50 of which are duplicates with randomly-generated errors.
In the code block, below we load the package and examine the first few rows 
of `RLdata500`. 
```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(magrittr)    # pipe operator (must be loaded before BDD)
library(comparator)  # normalized Levenshtein distance
library(clevr)       # evaluation functions
library(BDD)

RLdata500[['rec_id']] <- seq.int(nrow(RLdata500))
head(RLdata500)
```

The model uses agreement levels between attributes in order to assess 
whether a pair of records is a match (referring to the same entity) or not.
To compute the agreement levels, we'll first compute real-valued scores
between the attributes, then maps the scores to discrete levels of agreement.
In the code block below, we specify scoring functions for the five attributes 
we will use for matching (we don't use `fname_c2` and `lname_c2` as more than 
90% of the values are missing). 
Note that the scoring functions must accept a pair of attribute vectors `x` 
and `y` as arguments and return a vector of scores.
```{r, eval=TRUE}
scoring_fns <- list(
  fname_c1 = Levenshtein(normalize = TRUE),
  lname_c1 = Levenshtein(normalize = TRUE),
  by = function(x, y) abs(x - y),
  bm = function(x, y) abs(x - y),
  bd = function(x, y) abs(x - y)
)
```

For each scoring function above, we provide a breaks vectors which 
specifies the discrete levels of agreement (from 'high' agreement to 'low').
```{r, eval=TRUE}
scoring_breaks <- list(
  fname_c1 = c(-Inf, .05, .2, .4, Inf),
  lname_c1 = c(-Inf, .05, .2, .4, Inf),
  by = c(-Inf, 0, 1, 3, Inf),
  bm = c(-Inf, 0, 1, 3, Inf),
  bd = c(-Inf, 0, 2, 7, Inf)
)
```

Now we are ready to compute the agreement levels for the record pairs. 
Since this is a small data set, we consider all pairs using the `pairs_all` 
function. 
For larger data sets, blocking/indexing is recommended using `pairs_hamming`, 
`pairs_fuzzyblock` or a custom indexing function.
```{r, eval=TRUE}
pairs <- pairs_all(RLdata500$rec_id) %>%
  compute_scores(RLdata500, scoring_fns, id_col = 'rec_id') %>% 
  discretize_scores(scoring_breaks)
```

To speed up inference, we only consider a subset of the pairs as candidate 
matches. 
Specifically, we consider pairs that have a strong agreement on name 
(accounting for missing names).
```{r, eval=TRUE}
pairs[['candidate']] <- (pairs$fname_c1 < 4) & (pairs$lname_c1 < 4) |
                            is.na(pairs$fname_c1) | is.na(pairs$lname_c1) 
```

Next we specify the priors on the m* and u* probabilities for each 
attribute and agreement level. 
`lambda` specifies the lower truncation points for the truncated Beta priors 
on the m* probabilities. 
By default, a uniform prior is used over the truncated interval; a non-uniform 
prior can be specified using the `alpha1` and `beta1` arguments to `BDD` 
below. 
The Beta priors on the u* probabilities are also uniform by default and 
can be adjusted using the `alpha0` and `beta0` arguments.
```{r, eval=TRUE, message=FALSE, warning=FALSE}
lambda <- list(
  fname_c1 = c(0.8, 0.85, 0.99),
  lname_c1 = c(0.8, 0.85, 0.99),
  by = c(0.8, 0.85, 0.99),
  bm = c(0.8, 0.85, 0.99),
  bd = c(0.8, 0.85, 0.99)
)
```

Finally, we initialize the model and run inference using Markov chain 
Monte Carlo.
```{r, eval=TRUE}
model <- BDD(pairs, lambda, id_cols = c("rec_id.x", "rec_id.y"), 
             candidate_col = "candidate")
fit <- run_inference(model, 100, thin_interval = 10, burnin_interval = 100)
```

The posterior samples of the linkage structure can be accessed by calling 
`extract(fit, "links")`. 
However, these samples only cover the records that were considered as 
candidate pairs. 
We can obtain samples of the complete linkage structure (for all records) 
using the following function.
```{r, eval=TRUE}
links_samples <- complete_links_samples(fit, RLdata500$rec_id)
```

We recommend inspecting trace plots to verify that the Markov chain has 
reached equilibrium and is mixing well. The results below seem acceptable given 
the small number of samples.
```{r, eval=FALSE}
n_clusters <- extract(fit, "n_clusters")
m_probs <- extract(fit, "m")
u_probs <- extract(fit, "u")
plot(n_clusters)
plot(m_probs)
plot(u_probs)
```

Since RLdata500 comes with ground truth, we can evaluate the predicted 
linkage structure using supervised metrics. 
We find that the model performs well on this data set, achieving high 
pairwise precision and recall.
```{r, eval=TRUE}
n_records <- nrow(RLdata500)
true_pairs <- membership_to_pairs(identity.RLdata500)
metrics <- apply(links_samples, 1, function(links) {
  pred_pairs <- membership_to_pairs(links)
  pr <- precision_pairs(true_pairs, pred_pairs)
  re <- recall_pairs(true_pairs, pred_pairs)
  c(Precision = pr, Recall = re)
}) %>% t()
boxplot(metrics)
```
